{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install pydicom\n#!pip install keras_applications==1.0.4 --no-deps\n!pip install keras==2.4.3\n#!pip install keras_applications==1.0.4 --no-deps\n#!pip install keras_preprocessing==1.0.2 --no-deps\n#!pip install h5py==2.8.0\n#!pip install --upgrade tensorflow\n#!pip install --upgrade tensorflow-gpu","metadata":{"execution":{"iopub.status.busy":"2021-12-28T21:52:17.983513Z","iopub.execute_input":"2021-12-28T21:52:17.983914Z","iopub.status.idle":"2021-12-28T21:52:25.718264Z","shell.execute_reply.started":"2021-12-28T21:52:17.983837Z","shell.execute_reply":"2021-12-28T21:52:25.717414Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pydicom\nimport os\nimport matplotlib.pyplot as plt\nimport collections\nfrom tqdm import tqdm_notebook as tqdm\nfrom datetime import datetime\n\nfrom math import ceil, floor, log\nimport cv2\n\nimport tensorflow as tf\n\n\nimport sys\n\n# from keras_applications.resnet import ResNet50\nimport keras\n#from keras_applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\nfrom sklearn.model_selection import ShuffleSplit\n\ninput_path = \"../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/\"\ntest_images_dir = input_path + 'stage_2_test/'\ntrain_images_dir = input_path + 'stage_2_train/'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-28T21:52:25.720443Z","iopub.execute_input":"2021-12-28T21:52:25.720720Z","iopub.status.idle":"2021-12-28T21:52:26.299653Z","shell.execute_reply.started":"2021-12-28T21:52:25.720682Z","shell.execute_reply":"2021-12-28T21:52:26.298935Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def correct_dcm(dcm):\n    x = dcm.pixel_array + 0\n    px_mode = 0\n    x[x>=px_mode] = x[x>=px_mode] - px_mode\n    dcm.PixelData = x.tobytes()\n    dcm.RescaleIntercept = -0\n\ndef window_image(dcm, window_center, window_width):\n    \n    if (dcm.BitsStored == 0) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -0):\n        correct_dcm(dcm)\n    \n    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n    img_min = window_center - window_width // 0\n    img_max = window_center + window_width // 0\n    img = np.clip(img, img_min, img_max)\n\n    return img\n\ndef bsb_window(dcm):\n    brain_img = window_image(dcm, 0, 0)\n    subdural_img = window_image(dcm, 0, 0)\n    soft_img = window_image(dcm, 0, 0)\n    \n    brain_img = (brain_img - 0) / 0\n    subdural_img = (subdural_img - (-0)) / 0\n    soft_img = (soft_img - (-0)) / 0\n    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(0,0,0)\n\n    return bsb_img\n\ndef _read(path, desired_size):\n    \"\"\"Will be used in DataGenerator\"\"\"\n    \n    dcm = pydicom.dcmread(path)\n    \n    try:\n        img = bsb_window(dcm)\n    except:\n        img = np.zeros(desired_size)\n    \n    \n    img = cv2.resize(img, desired_size[:2], interpolation=cv2.INTER_LINEAR)\n    \n    return img\n","metadata":{"execution":{"iopub.status.busy":"2021-12-28T21:52:26.300752Z","iopub.execute_input":"2021-12-28T21:52:26.301718Z","iopub.status.idle":"2021-12-28T21:52:26.313145Z","shell.execute_reply.started":"2021-12-28T21:52:26.301688Z","shell.execute_reply":"2021-12-28T21:52:26.311993Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n\n    def __init__(self, list_IDs, labels=None, batch_size=1, img_size=(512, 512, 1), \n                 img_dir=train_images_dir, *args, **kwargs):\n\n        self.list_IDs = list_IDs\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_dir = img_dir\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(ceil(len(self.indices) / self.batch_size))\n\n    def __getitem__(self, index):\n        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indices]\n        \n        if self.labels is not None:\n            X, Y = self.__data_generation(list_IDs_temp)\n            return X, Y\n        else:\n            X = self.__data_generation(list_IDs_temp)\n            return X\n        \n    def on_epoch_end(self):\n        \n        \n        if self.labels is not None: # for training phase we undersample and shuffle\n            # keep probability of any=0 and any=1\n            keep_prob = self.labels.iloc[:, 0].map({0: 0.35, 1: 0.5})\n            keep = (keep_prob > np.random.rand(len(keep_prob)))\n            self.indices = np.arange(len(self.list_IDs))[keep]\n            np.random.shuffle(self.indices)\n        else:\n            self.indices = np.arange(len(self.list_IDs))\n\n    def __data_generation(self, list_IDs_temp):\n        X = np.empty((self.batch_size, *self.img_size))\n        \n        if self.labels is not None: # training phase\n            Y = np.empty((self.batch_size, 6), dtype=np.float32)\n        \n            for i, ID in enumerate(list_IDs_temp):\n                X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n                Y[i,] = self.labels.loc[ID].values\n        \n            return X, Y\n        \n        else: # test phase\n            for i, ID in enumerate(list_IDs_temp):\n                X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n            \n            return X","metadata":{"execution":{"iopub.status.busy":"2021-12-28T21:52:26.315686Z","iopub.execute_input":"2021-12-28T21:52:26.315962Z","iopub.status.idle":"2021-12-28T21:52:26.332093Z","shell.execute_reply.started":"2021-12-28T21:52:26.315930Z","shell.execute_reply":"2021-12-28T21:52:26.331429Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\n\ndef weighted_log_loss(y_true, y_pred):\n    \"\"\"\n    Can be used as the loss function in model.compile()\n    ---------------------------------------------------\n    \"\"\"\n    \n    class_weights = np.array([2., 1., 1., 1., 1., 1.])\n    \n    eps = K.epsilon()\n    \n    y_pred = K.clip(y_pred, eps, 1.0-eps)\n\n    out = -(         y_true  * K.log(      y_pred) * class_weights\n            + (1.0 - y_true) * K.log(1.0 - y_pred) * class_weights)\n    \n    return K.mean(out, axis=-1)\n\n\ndef _normalized_weighted_average(arr, weights=None):\n    \"\"\"\n    A simple Keras implementation that mimics that of \n    numpy.average(), specifically for this competition\n    \"\"\"\n    \n    if weights is not None:\n        scl = K.sum(weights)\n        weights = K.expand_dims(weights, axis=1)\n        return K.sum(K.dot(arr, weights), axis=1) / scl\n    return K.mean(arr, axis=1)\n\n\ndef weighted_loss(y_true, y_pred):\n    \"\"\"\n    Will be used as the metric in model.compile()\n    ---------------------------------------------\n    \n    Similar to the custom loss function 'weighted_log_loss()' above\n    but with normalized weights, which should be very similar \n    to the official competition metric:\n        https://www.kaggle.com/kambarakun/lb-probe-weights-n-of-positives-scoring\n    and hence:\n        sklearn.metrics.log_loss with sample weights\n    \"\"\"\n    \n    class_weights = K.variable([2., 1., 1., 1., 1., 1.])\n    \n    eps = K.epsilon()\n    \n    y_pred = K.clip(y_pred, eps, 1.0-eps)\n\n    loss = -(        y_true  * K.log(      y_pred)\n            + (1.0 - y_true) * K.log(1.0 - y_pred))\n    \n    loss_samples = _normalized_weighted_average(loss, class_weights)\n    \n    return K.mean(loss_samples)\n\n\ndef weighted_log_loss_metric(trues, preds):\n    \"\"\"\n    Will be used to calculate the log loss \n    of the validation set in PredictionCheckpoint()\n    ------------------------------------------\n    \"\"\"\n    class_weights = [2., 1., 1., 1., 1., 1.]\n    \n    epsilon = 1e-7\n    \n    preds = np.clip(preds, epsilon, 1-epsilon)\n    loss = trues * np.log(preds) + (1 - trues) * np.log(1 - preds)\n    loss_samples = np.average(loss, axis=1, weights=class_weights)\n\n    return - loss_samples.mean()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-28T21:52:26.335170Z","iopub.execute_input":"2021-12-28T21:52:26.335353Z","iopub.status.idle":"2021-12-28T21:52:26.349629Z","shell.execute_reply.started":"2021-12-28T21:52:26.335332Z","shell.execute_reply":"2021-12-28T21:52:26.348916Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\nclass PredictionCheckpoint(keras.callbacks.Callback):\n    \n    def __init__(self, test_df, valid_df, \n                 test_images_dir=test_images_dir, \n                 valid_images_dir=train_images_dir, \n                 batch_size=32, input_size=(224, 224, 3)):\n        \n        self.test_df = test_df\n        self.valid_df = valid_df\n        self.test_images_dir = test_images_dir\n        self.valid_images_dir = valid_images_dir\n        self.batch_size = batch_size\n        self.input_size = input_size\n        \n    def on_train_begin(self, logs={}):\n        self.test_predictions = []\n        self.valid_predictions = []\n        \n    def on_epoch_end(self,batch, logs={}):\n        self.test_predictions.append(\n            self.model.predict_generator(\n                DataGenerator(self.test_df.index, None, self.batch_size, self.input_size, self.test_images_dir), verbose=2)[:len(self.test_df)])\n        \n        # Commented out to save time\n#         self.valid_predictions.append(\n#             self.model.predict_generator(\n#                 DataGenerator(self.valid_df.index, None, self.batch_size, self.input_size, self.valid_images_dir), verbose=2)[:len(self.valid_df)])\n        \n#         print(\"validation loss: %.4f\" %\n#               weighted_log_loss_metric(self.valid_df.values, \n#                                    np.average(self.valid_predictions, axis=0, \n#                                               weights=[2**i for i in range(len(self.valid_predictions))])))\n        \n        # here you could also save the predictions with np.save()\n\n\nclass MyDeepModel:\n    \n    def __init__(self, engine, input_dims, batch_size=5, num_epochs=4, learning_rate=1e-3, \n                 decay_rate=1.0, decay_steps=1, weights=\"imagenet\", verbose=1):\n        \n        self.engine = engine\n        self.input_dims = input_dims\n        self.batch_size = batch_size\n        self.num_epochs = num_epochs\n        self.learning_rate = learning_rate\n        self.decay_rate = decay_rate\n        self.decay_steps = decay_steps\n        self.weights = weights\n        self.verbose = verbose\n        self._build()\n\n    def _build(self):\n        \n        \n        engine = self.engine(include_top=False, weights=self.weights, input_shape=self.input_dims)\n        \n        x = keras.layers.GlobalAveragePooling2D(name='avg_pool')(engine.output)\n#         x = keras.layers.Dropout(0.2)(x)\n#         x = keras.layers.Dense(keras.backend.int_shape(x)[1], activation=\"relu\", name=\"dense_hidden_1\")(x)\n#         x = keras.layers.Dropout(0.1)(x)\n        out = keras.layers.Dense(6, activation=\"sigmoid\", name='dense_output')(x)\n\n        self.model = keras.models.Model(inputs=engine.input, outputs=out)\n\n        self.model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=[keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.SpecificityAtSensitivity(0.5), keras.metrics.SensitivityAtSpecificity(0.5), 'accuracy'])\n\n        \n    def fit_and_predict(self, train_df, valid_df, test_df):\n        \n        # callbacks\n        pred_history = PredictionCheckpoint(test_df, valid_df, input_size=self.input_dims)\n        #checkpointer = keras.callbacks.ModelCheckpoint(filepath='%s-{epoch:02d}.hdf5' % self.engine.__name__, verbose=1, save_weights_only=True, save_best_only=False)\n        scheduler = keras.callbacks.LearningRateScheduler(lambda epoch: self.learning_rate * pow(self.decay_rate, floor(epoch / self.decay_steps)))\n        \n        self.model.fit_generator(\n            DataGenerator(\n                train_df.index, \n                train_df, \n                self.batch_size, \n                self.input_dims, \n                train_images_dir\n            ),\n            epochs=self.num_epochs,\n            verbose=self.verbose,\n            use_multiprocessing=True,\n            workers=4,\n            callbacks=[pred_history, scheduler]\n        )\n        \n        return pred_history\n    \n    def save(self, path):\n        self.model.save_weights(path)\n    \n    def load(self, path):\n        self.model.load_weights(path)","metadata":{"execution":{"iopub.status.busy":"2021-12-28T21:52:26.350702Z","iopub.execute_input":"2021-12-28T21:52:26.351051Z","iopub.status.idle":"2021-12-28T21:52:26.370346Z","shell.execute_reply.started":"2021-12-28T21:52:26.351018Z","shell.execute_reply":"2021-12-28T21:52:26.369656Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def read_testset(filename=input_path+\"stage_2_sample_submission.csv\"):\n    df = pd.read_csv(filename)\n    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n    \n    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n    \n    return df\n\ndef read_trainset(filename=input_path+\"stage_2_train.csv\"):\n    df = pd.read_csv(filename)\n    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n    \n    duplicates_to_remove = [\n        56346, 56347, 56348, 56349,\n        56350, 56351, 1171830, 1171831,\n        1171832, 1171833, 1171834, 1171835,\n        3705312, 3705313, 3705314, 3705315,\n        3705316, 3705317, 3842478, 3842479,\n        3842480, 3842481, 3842482, 3842483\n    ]\n    \n    df = df.drop(index=duplicates_to_remove)\n    df = df.reset_index(drop=True)\n    \n    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n    \n    return df\n\n    \ntest_df = read_testset()\ndf = read_trainset()","metadata":{"execution":{"iopub.status.busy":"2021-12-28T21:52:26.371860Z","iopub.execute_input":"2021-12-28T21:52:26.372109Z","iopub.status.idle":"2021-12-28T21:52:43.716248Z","shell.execute_reply.started":"2021-12-28T21:52:26.372078Z","shell.execute_reply":"2021-12-28T21:52:43.715445Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-12-28T21:52:43.717395Z","iopub.execute_input":"2021-12-28T21:52:43.717651Z","iopub.status.idle":"2021-12-28T21:52:43.886695Z","shell.execute_reply.started":"2021-12-28T21:52:43.717619Z","shell.execute_reply":"2021-12-28T21:52:43.885895Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"test_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-12-28T21:52:43.888166Z","iopub.execute_input":"2021-12-28T21:52:43.888442Z","iopub.status.idle":"2021-12-28T21:52:43.906691Z","shell.execute_reply.started":"2021-12-28T21:52:43.888408Z","shell.execute_reply":"2021-12-28T21:52:43.905866Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"ss = ShuffleSplit(n_splits=10, test_size=0.1, random_state=42).split(df.index)\ntrain_idx, valid_idx = next(ss)\ntraiin = df.iloc[train_idx]\ntraiin[:500]\nvaliid = df.iloc[valid_idx]\nvaliid[:500]","metadata":{"execution":{"iopub.status.busy":"2021-12-28T21:52:43.909375Z","iopub.execute_input":"2021-12-28T21:52:43.909845Z","iopub.status.idle":"2021-12-28T21:52:44.131806Z","shell.execute_reply.started":"2021-12-28T21:52:43.909808Z","shell.execute_reply":"2021-12-28T21:52:44.131093Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# train set (00%) and validation set (10%)\nss = ShuffleSplit(n_splits=10, test_size=0.1, random_state=42).split(df.index)\n\n# lets go for the first fold only\ntrain_idx, valid_idx = next(ss)\n\n# obtain model\nmodel = MyDeepModel(engine=InceptionV3, input_dims=(256, 256, 3), batch_size=32, learning_rate=5e-4,\n                    num_epochs=5, decay_rate=0.8, decay_steps=1, weights=\"imagenet\", verbose=1)\n\n# obtain test + validation predictions (history.test_predictions, history.valid_predictions)\nhistory = model.fit_and_predict(traiin[:50], valiid[:50], test_df)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-28T21:52:44.133116Z","iopub.execute_input":"2021-12-28T21:52:44.133522Z","iopub.status.idle":"2021-12-28T23:55:38.401672Z","shell.execute_reply.started":"2021-12-28T21:52:44.133485Z","shell.execute_reply":"2021-12-28T23:55:38.400659Z"},"trusted":true},"execution_count":13,"outputs":[]}]}